{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "def dump(data, name):\n",
    "    filehandler = open(name, \"wb\")\n",
    "    pickle.dump(data, filehandler)\n",
    "    filehandler.close()\n",
    "def load(name):\n",
    "    filehandler = open(name, \"rb\")\n",
    "    return pickle.load(filehandler)\n",
    "\n",
    "def find_leaf_node(tag):\n",
    "    if tag.string!=None and tag.string!=' ' and len(list(tag.children))==1: # leaf node인지\n",
    "        if '/' in list(tag.contents[0]) and len(tag.contents[0].split())!=1: # 형태소 분석된 정보인지\n",
    "            return tag\n",
    "\n",
    "def isPOStag(entry):\n",
    "    # 세종 코퍼스 tagset은 총 45개이다. 하지만 여기서 코퍼스에서 존재하지 않는 'NF'와 'NV'는 제거하였다.\n",
    "    # 나중에 'START'와 'END' tag를 추가하면 어차피 45개가 된다.\n",
    "    tagSet = ['NNB', 'SH', 'VCN', 'EP', 'SF', 'NR', 'VCP', 'SW', 'XSN', 'JKV', 'XPN', 'MAJ', 'NP', 'MM', 'EF', 'SO', 'VX', 'EC', 'JKO', 'XR', 'NNG', 'JKG', 'JKC', 'SE', 'VA', 'IC', 'SN', 'SS', 'JKS', 'JC', 'JKQ', 'XSV', 'ETM', 'MAG', 'ETN', 'JKB', 'VV', 'SL', 'SP', 'JX', 'XSA', 'NA', 'NNP']\n",
    "    if entry.split('/')[-1] in tagSet:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# def exception_check(entry):\n",
    "#     ### Exception 1: 만약 '/'를 제외하고 모두 digit이라면? -> 형태소 분석 정보가 아니므로 삭제할 필요가 있다.\n",
    "#     tkn = False\n",
    "#     splited_entry = entry.split('/')\n",
    "#     #print(splited_entry)\n",
    "#     for spl_ent in splited_entry:\n",
    "#         #print(spl_ent)\n",
    "#         if (spl_ent).isdigit() == False: # digit가 한개라도 없으면 아무 이상 없다.\n",
    "#             tkn = True\n",
    "     \n",
    "#     ### Exception 2: 만약 '/'를 제외하고 모두 한글이라면? -> 형태소 분석 정보(ex. 자유문고/NNP)가 아니므로 삭제할 필요가 있다.\n",
    "#     alphabet = ['A','B','C','D','E','F','G','H','I','G','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "#     splited_entry = entry.split('/')\n",
    "#     tkn2 = False\n",
    "    \n",
    "#     for spl_ent in splited_entry:\n",
    "#         for char in list(spl_ent):\n",
    "#             if char in alphabet: # 영어 알파벳이 한개라도 있으면 아무 이상 없다.\n",
    "#                 tkn2 = True    \n",
    "    \n",
    "#     if tkn == False or tkn2 == False: \n",
    "#         return False\n",
    "#     else:\n",
    "#         return True # exception check해보았으나 아무 이상 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sources from Sejong Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# allTag_List = []\n",
    "# allWord_List = []\n",
    "# allTagWord_List = []\n",
    "# allpTagcTag_List = []\n",
    "\n",
    "# path = os.getcwd()+'\\\\saejongcorpus'\n",
    "# dirs = os.listdir(path)\n",
    "\n",
    "# for file in dirs:\n",
    "#     # read\n",
    "#     with open(path+'\\\\'+file, \"rt\", encoding='UTF8') as markup:\n",
    "#         soup = BeautifulSoup(markup.read(), \"lxml\")\n",
    "    \n",
    "#     # working\n",
    "#     collection_leafNodes = soup.find_all(find_leaf_node)\n",
    "    \n",
    "#     for leaf_node in collection_leafNodes: # <date>, <p> 와 같은 leaf node들을 traverse하겠다.\n",
    "\n",
    "#         #preTag = 'START'\n",
    "#         for i, entry in enumerate(leaf_node.contents[0].split()): # 그냥 공백으로 나눈 것들을 entry라고 하자.\n",
    "\n",
    "#             if isPOStag(entry) == True: # BSDO0276-00000009, 목젖으로, 목젖/NNG, +, 으로/JKB 에서 목젖/NNG과 으로/JKB만 걸러내겠다.\n",
    "                \n",
    "#                 ### 재료 준비\n",
    "#                 tag = entry.split('/')[-1] # tag는 entry 마지막\n",
    "#                 if len(entry.split('/')) == 3: # '/' 이것 때문에 예외처리하는 것이다. (ex //SP인 경우)\n",
    "#                     word = '/'\n",
    "#                 else:\n",
    "#                     word = entry.split('/')[0] # word는 entry 첫번째\n",
    "                \n",
    "#                 # 의사__12/NNG -> '의사'로 처리해주기 위해...\n",
    "#                 word = word.split('_')[0]\n",
    "                \n",
    "#                 # smoothing을 위해 모든 단어가 있는 voca 만듦\n",
    "#                 allWord_List.append(word)\n",
    "            \n",
    "            \n",
    "#                 #######################\n",
    "#                 \"\"\" (tag)만 추출하자 \"\"\"\n",
    "#                 ####################### \n",
    "#                 allTag_List.append(tag)\n",
    "                \n",
    "#                 # START, END tag 만들어주기\n",
    "#                 if i == 2:# i=0일 때는 BSAA0002-00000001이다. / i=1일 때는 좌우 / i=2일 때는 좌우__01/NNG\n",
    "#                     allTag_List.append('START') # START tag\n",
    "#                 elif i == len(leaf_node.contents[0].split())-1:\n",
    "#                     allTag_List.append('END') # END tag            \n",
    "                \n",
    "                \n",
    "#                 ############################\n",
    "#                 \"\"\" (tag, word)를 추출하자 \"\"\"\n",
    "#                 ############################\n",
    "#                 tagword_pair = (tag, word) \n",
    "#                 allTagWord_List.append(tagword_pair)\n",
    "                \n",
    "                \n",
    "#                 ###################################\n",
    "#                 \"\"\" (pre_tag, cur_tag)를 추출하자 \"\"\"\n",
    "#                 ###################################\n",
    "#                 if i == 2: # 아마 여기가 제일 처음일 것이다.\n",
    "#                     pTagcTag_pair = ('START', tag)\n",
    "#                     allpTagcTag_List.append(pTagcTag_pair)\n",
    "#                     preTag = tag # 현재 tag를 할당\n",
    "#                 elif i == len(leaf_node.contents[0].split())-1:\n",
    "#                     pTagcTag_pair = (tag, 'END')\n",
    "#                     allpTagcTag_List.append(pTagcTag_pair)\n",
    "#                 else:\n",
    "#                     pTagcTag_pair = (preTag, tag)\n",
    "#                     allpTagcTag_List.append(pTagcTag_pair)\n",
    "\n",
    "\n",
    "#         #print('\\n\\n')\n",
    "        \n",
    "\n",
    "    \n",
    "#     #break\n",
    "\n",
    "    \n",
    "# # ### DUMP\n",
    "# # dump(allTag_List, 'allTag_List')\n",
    "# # dump(allWord_List, 'allWord_List')\n",
    "# # dump(allTagWord_List, 'allTagWord_List')\n",
    "# # dump(allpTagcTag_List, 'allpTagcTag_List')\n",
    "\n",
    "# ### Make Dictionary\n",
    "# # Word\n",
    "# freq_allWord = dict()\n",
    "# for i in allWord_List:\n",
    "#     freq_allWord[i] = freq_allWord.get(i, 0) + 1\n",
    "# # Tag\n",
    "# freq_allTag = dict()\n",
    "# for i in allTag_List:\n",
    "#     freq_allTag[i] = freq_allTag.get(i, 0) + 1\n",
    "# # Tag, Word\n",
    "# freq_allTagWord = dict()\n",
    "# for i in allTagWord_List:\n",
    "#     freq_allTagWord[i] = freq_allTagWord.get(i, 0) + 1\n",
    "# # pTag, cTag     \n",
    "# freq_allpTagcTag = dict()\n",
    "# for i in allpTagcTag_List:\n",
    "#     freq_allpTagcTag[i] = freq_allpTagcTag.get(i, 0) + 1\n",
    "    \n",
    "# ### DUMP\n",
    "# dump(freq_allTag, 'freq_allTag')\n",
    "# dump(freq_allTagWord, 'freq_allTagWord')\n",
    "# dump(freq_allWord, 'freq_allWord')\n",
    "# dump(freq_allpTagcTag, 'freq_allpTagcTag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# freq_Tag = load('storage/freq_allTag')\n",
    "# freq_TagWord = load('storage/freq_allTagWord') # output probability를 구하기 위해\n",
    "# freq_Word = load('storage/freq_allWord')\n",
    "# freq_pTagcTag = load('storage/freq_allpTagcTag') # transition probability를 구하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# n = 0\n",
    "# for key in freq_pTagcTag.items():\n",
    "#     print(key)\n",
    "#     n += 1\n",
    "#     if n==10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def is_inDict(tag, word, dict):\n",
    "#     try:\n",
    "#         dict[(tag,word)]\n",
    "#     except(KeyError):\n",
    "#         return True # 사전에 없다\n",
    "#     return False\n",
    "\n",
    "# ### (Tag, Word) Smoothing\n",
    "# # (Tag, Word)를 smoothing하기 위해 word voca를 구축.\n",
    "# for tag in freq_Tag:\n",
    "#     for word in freq_Word:\n",
    "#         if is_inDict(tag, word, freq_TagWord) == True: # 사전에 없는 경우\n",
    "#             freq_TagWord[(tag, word)] = 1 # smoothing으로 1 추가\n",
    "# # After smoothing...\n",
    "# # len(freq_allTagWord) = 9202185 <---------------- 너무 큰 것 아님........?????\n",
    "\n",
    "# ### (prevTag, curTag) Smoothing\n",
    "# for tag1 in freq_Tag:\n",
    "#     for tag2 in freq_Tag:\n",
    "#         if is_inDict(tag1, tag2, freq_pTagcTag) == True: # 사전에 없는 경우\n",
    "#             freq_pTagcTag[(tag1, tag2)] = 1 # smoothing으로 1 추가   \n",
    "# # Before smoothing\n",
    "# # len(freq_allpTagcTag) = 1287\n",
    "# # After smoothing\n",
    "# # len(freq_allpTagcTag) = 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ### p( Word | Tag )\n",
    "# prob_TagWord = freq_TagWord\n",
    "# for pos, _ in freq_Tag.items():\n",
    "#     # for each Tag\n",
    "#     temp = dict((k, v) for k, v in freq_TagWord.items() if k[0] == pos)\n",
    "#     total = sum(temp.values())\n",
    "#     for pair, value in temp.items():\n",
    "#         prob_TagWord[pair] = value / total\n",
    "        \n",
    "# n = 0\n",
    "# for key in prob_TagWord.items():\n",
    "#     print(key)\n",
    "#     n += 1\n",
    "#     if n==10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### p( cTag | pTag )\n",
    "# prob_pTagcTag = freq_pTagcTag\n",
    "# for pTag, _ in freq_Tag.items():\n",
    "#     # for each previous Tag\n",
    "#     temp = dict((k, v) for k, v in freq_pTagcTag.items() if k[0] == pTag) # k[0]: previous tag\n",
    "#     total = sum(temp.values())\n",
    "#     for pair, value in temp.items():\n",
    "#         prob_pTagcTag[pair] = value / total  \n",
    "\n",
    "# n = 0\n",
    "# for key in prob_pTagcTag.items():\n",
    "#     print(key)\n",
    "#     n += 1\n",
    "#     if n==10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_Tag = load('storage/freq_Tag')\n",
    "freq_TagWord = load('storage/freq_TagWord') # output probability를 구하기 위해\n",
    "freq_Word = load('storage/freq_Word')\n",
    "freq_pTagcTag = load('storage/freq_pTagcTag') # transition probability를 구하기 위해\n",
    "\n",
    "prob_pTagcTag = load('storage/prob_pTagcTag')\n",
    "prob_TagWord = load('storage/prob_TagWord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66760"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pTagcTag['START', 'SS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32310500167395817"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_pTagcTag['START', 'NNG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228727"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pTagcTag['START', 'NNG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228727"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pTagcTag['START', 'NNG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32310500167395817"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_pTagcTag['START', 'NNG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_TagWord[('NNG', '우선')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2478"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_TagWord[('MAG', '우선')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a= prob_TagWord['NNG', '을']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=prob_TagWord['JKO', '을']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c=prob_pTagcTag['NNG','NNG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=prob_pTagcTag['NNG','JKO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ho\n"
     ]
    }
   ],
   "source": [
    "if a*c < b*d:\n",
    "    print('ho')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From morphological analysis (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = '신고전주의정신을' \n",
    "morph_result = [[['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고', 3, 4, 'ENT', 'NNG NounV 고/NNG'], ['전', 5, 7, 'ENT', 'NNG NounC 전/NNG'], ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'], ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고', 3, 4, 'ENT', 'NNG NounV 고/NNG'], ['전', 5, 7, 'ENT', 'NNG NounC 전/NNG'], ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고', 3, 4, 'ENT', 'NNG NounV 고/NNG'], ['전주', 5, 9, 'ENT', 'NNG NounV 전주/NNG'], ['의', 10, 11, 'ENT', 'NNG NounV 의/NNG'], ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고', 3, 4, 'ENT', 'NNG NounV 고/NNG'], ['전주', 5, 9, 'ENT', 'NNG NounV 전주/NNG'], ['의', 10, 11, 'ENT', 'NNG NounV 의/NNG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고', 3, 4, 'ENT', 'NNG NounV 고/NNG'], ['전주', 5, 9, 'ENT', 'NNG NounV 전주/NNG'], ['의정', 10, 14, 'ENT', 'NNG NounC 의정/NNG'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'VV VERB-REG 신/VV', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고', 3, 4, 'ENT', 'NNG NounV 고/NNG'], ['전주', 5, 9, 'ENT', 'NNG NounV 전주/NNG'], ['의', 10, 11, 'FUN', 'J N-1-0 의/JKG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고전', 3, 7, 'ENT', 'NNG NounC 고전/NNG'], ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'], ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고전', 3, 7, 'ENT', 'NNG NounC 고전/NNG'], ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고전주의', 3, 11, 'ENT', 'NNG NounV 고전주의/NNG'], ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'], ['고전주의', 3, 11, 'ENT', 'NNG NounV 고전주의/NNG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신고', 0, 4, 'ENT', 'NNG NounV 신고/NNG'], ['전', 5, 7, 'ENT', 'NNG NounC 전/NNG'], ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'], ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신고', 0, 4, 'ENT', 'NNG NounV 신고/NNG'], ['전', 5, 7, 'ENT', 'NNG NounC 전/NNG'], ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신고', 0, 4, 'ENT', 'NNG NounV 신고/NNG'], ['전주', 5, 9, 'ENT', 'NNG NounV 전주/NNG'], ['의', 10, 11, 'ENT', 'NNG NounV 의/NNG'], ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신고', 0, 4, 'ENT', 'NNG NounV 신고/NNG'], ['전주', 5, 9, 'ENT', 'NNG NounV 전주/NNG'], ['의', 10, 11, 'ENT', 'NNG NounV 의/NNG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신고', 0, 4, 'ENT', 'NNG NounV 신고/NNG'], ['전주', 5, 9, 'ENT', 'NNG NounV 전주/NNG'], ['의정', 10, 14, 'ENT', 'NNG NounC 의정/NNG'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'VV VERB-REG 신/VV', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신고', 0, 4, 'ENT', 'NNG NounV 신고/NNG'], ['전주', 5, 9, 'ENT', 'NNG NounV 전주/NNG'], ['의', 10, 11, 'FUN', 'J N-1-0 의/JKG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신', 0, 2, 'FUN', 'E VA-1-0 시/EP+ㄴ/ETM'], ['고전', 3, 7, 'ENT', 'NNG NounC 고전/NNG'], ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'], ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신', 0, 2, 'FUN', 'E VA-1-0 시/EP+ㄴ/ETM'], ['고전', 3, 7, 'ENT', 'NNG NounC 고전/NNG'], ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']], [['신', 0, 2, 'FUN', 'E VA-1-0 시/EP+ㄴ/ETM'], ['고전주의', 3, 11, 'ENT', 'NNG NounV 고전주의/NNG'], ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'], ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'], ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']], [['신', 0, 2, 'FUN', 'E VA-1-0 시/EP+ㄴ/ETM'], ['고전주의', 3, 11, 'ENT', 'NNG NounV 고전주의/NNG'], ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'], ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph_result = [[['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'],\n",
    "  ['고', 3, 4, 'ENT', 'NNG NounV 고/NNG'],\n",
    "  ['전', 5, 7, 'ENT', 'NNG NounC 전/NNG'],\n",
    "  ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'],\n",
    "  ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'],\n",
    "  ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'],\n",
    "  ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']],\n",
    " [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'],\n",
    "  ['고', 3, 4, 'ENT', 'NNG NounV 고/NNG'],\n",
    "  ['전', 5, 7, 'ENT', 'NNG NounC 전/NNG'],\n",
    "  ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'],\n",
    "  ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'],\n",
    "  ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']],\n",
    " [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'],\n",
    "  ['고전', 3, 7, 'ENT', 'NNG NounC 고전/NNG'],\n",
    "  ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'],\n",
    "  ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'],\n",
    "  ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']],\n",
    " [['신', 0, 2, 'ENT', 'NNG NounC 신/NNG', 'XPN NounC 신/XPN'],\n",
    "  ['고전주의', 3, 11, 'ENT', 'NNG NounV 고전주의/NNG'],\n",
    "  ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'],\n",
    "  ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']],\n",
    " [['신고', 0, 4, 'ENT', 'NNG NounV 신고/NNG'],\n",
    "  ['전', 5, 7, 'ENT', 'NNG NounC 전/NNG'],\n",
    "  ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'],\n",
    "  ['정', 12, 14, 'ENT', 'XSN NounC 정/XSN'],\n",
    "  ['신', 15, 17, 'ENT', 'NNG NounC 신/NNG', 'XSN NounC 신/XSN'],\n",
    "  ['을', 18, 20, 'FUN', 'E VA-1-3 을/ETM', 'J N-8-3 을/JKO']],\n",
    " [['신고', 0, 4, 'ENT', 'NNG NounV 신고/NNG'],\n",
    "  ['전', 5, 7, 'ENT', 'NNG NounC 전/NNG'],\n",
    "  ['주의', 8, 11, 'ENT', 'NNG NounV 주의/NNG'],\n",
    "  ['정신', 12, 17, 'ENT', 'NNG NounC 정신/NNG'],\n",
    "  ['을', 18, 20, 'FUN', 'J N-8-3 을/JKO']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# 형태소 조합이 20개인데, 이들을 모두 viterbi를 돌려 확률값을 구하고\n",
    "# 20개 중에 가장 높은 확률을 가지는 조합을 best로 선택한다.\n",
    "print(len(morph_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from morph_analysis import Morph\n",
    "from hangul_utils import split_syllables, join_jamos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orgin_morph_idx(morph_result, idx):\n",
    "    new_morph_result = [[]] * len(morph_result)\n",
    "    \n",
    "    for i, case in enumerate(morph_result):\n",
    "        str_idx = idx\n",
    "        temp = []\n",
    "        for entry in case:\n",
    "            for pair in entry[-1].split()[-1].split('+'):\n",
    "                org_morph = pair.split('/')[0]\n",
    "                len_syllables = len(split_syllables(org_morph))\n",
    "                temp.append([org_morph, str_idx, str_idx+len_syllables-1])\n",
    "                #print([org_morph, str_idx, str_idx+len_syllables-1])\n",
    "                str_idx += len_syllables\n",
    "        new_morph_result[i] = temp\n",
    "        #print('===')\n",
    "    return new_morph_result\n",
    "\n",
    "def check_equal(cur_entry, list):\n",
    "    for entry in list:\n",
    "        if cur_entry[0] == entry[0]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def original_morphems(morph_result):\n",
    "    uniqueEntry_list = []\n",
    "    \n",
    "    for case in morph_result:\n",
    "        for entry in case:\n",
    "            if check_equal(entry, uniqueEntry_list) == False:\n",
    "                uniqueEntry_list.append(entry)    \n",
    "\n",
    "    #sorted1 = sorted(uniqueEntry_list, key=lambda k: k[1])\n",
    "    sorted2 = sorted(uniqueEntry_list, key=lambda k: k[2])\n",
    "    return sorted2\n",
    "\n",
    "def prepare_for_viterbi(sent):\n",
    "    final_entry_list = []\n",
    "    str_idx = 0\n",
    "    space_idx = []\n",
    "        \n",
    "    for i, token in enumerate(sent.split()):\n",
    "        \n",
    "        morph_result = morph.extract(token)\n",
    "\n",
    "        final_entry_list += original_morphems(orgin_morph_idx(morph_result, str_idx))\n",
    "        str_idx = original_morphems(orgin_morph_idx(morph_result, str_idx))[-1][2]+1\n",
    "        \n",
    "        if not i==len(sent.split())-1:\n",
    "            space_idx.append(str_idx)\n",
    "        \n",
    "    return final_entry_list, space_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['좁혀', 0, 4, 'ENT', 'VV VERB-REG4 좁히/VV'],\n",
       "  ['ㅆ다', 5, 7, 'FUN', 'E V-5-0 었/EP+다/EF']]]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = Morph()\n",
    "morph.extract('좁혔다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['나', 0, 1, 'ENT', 'NP NounV 나/NP'],\n",
       "  ['느', 2, 3, 'ENT', 'VV VERB-L1 늘/VV'],\n",
       "  ['ㄴ', 4, 4, 'FUN', 'E VA-1-0 ㄴ/ETM']],\n",
       " [['나',\n",
       "   0,\n",
       "   1,\n",
       "   'ENT',\n",
       "   'NP NounV 나/NP',\n",
       "   'VV VERB-REG3 나/VV',\n",
       "   'VX VERB-REG3 나/VX',\n",
       "   'XPN NounV 나/XPN',\n",
       "   'VV VERB-L1 날/VV'],\n",
       "  ['는', 2, 4, 'FUN', 'E V-6-0 는/ETM', 'E VA-6-0 는/ETM', 'J N-7-2 는/JX']]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = Morph()\n",
    "morph.extract('나는')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['나', 0, 1], ['날', 0, 2], ['늘', 2, 4], ['ㄴ', 5, 5], ['는', 3, 5]]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '나는'\n",
    "prepare_for_viterbi(sent)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_viterbi(sent, morph_class):\n",
    "    \n",
    "    for i, token in enumerate(sent.split()):\n",
    "        morph_result = morph_class.extract(token)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph_result = morph.extract('나는')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_result[0][-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['나', 0, 1, 'ENT', 'NP NounV 나/NP'],\n",
       "  ['느', 2, 3, 'ENT', 'VV VERB-L1 늘/VV'],\n",
       "  ['ㄴ', 4, 4, 'FUN', 'E VA-1-0 ㄴ/ETM']],\n",
       " [['나',\n",
       "   0,\n",
       "   1,\n",
       "   'ENT',\n",
       "   'NP NounV 나/NP',\n",
       "   'VV VERB-REG3 나/VV',\n",
       "   'VX VERB-REG3 나/VX',\n",
       "   'XPN NounV 나/XPN',\n",
       "   'VV VERB-L1 날/VV'],\n",
       "  ['는', 2, 4, 'FUN', 'E V-6-0 는/ETM', 'E VA-6-0 는/ETM', 'J N-7-2 는/JX']]]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-a5009efe2535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;31m# sorting based on start index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfinal_collect_entry_anly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_collect_entry_anly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;31m# 오류확인 1. 처음과 끝의 경계는 아래와 같이 체크하면 된다. (중간에 있는 오류는 다음단계에서 확인한다.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-149-a5009efe2535>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;31m# sorting based on start index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfinal_collect_entry_anly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_collect_entry_anly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;31m# 오류확인 1. 처음과 끝의 경계는 아래와 같이 체크하면 된다. (중간에 있는 오류는 다음단계에서 확인한다.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "#final_collect_entry_anly = prepare_for_viterbi(sent)[0]\n",
    "#len_target_word = prepare_for_viterbi(sent)[0][-1][-1]\n",
    "final_collect_entry_anly = morph_result\n",
    "len_target_word = morph_result[0][-1][2]\n",
    "\n",
    "def recursive_merging(collection, index_set, path, list_global_cases):\n",
    "    temp_path = path[:]\n",
    "    for index in index_set:\n",
    "        path.append(index)\n",
    "        index_set = [e for e in collection if int(e[1]) == index[2]+1] # first position\n",
    "        if len(index_set) == 0: # the end point\n",
    "            # save to global and then game over\n",
    "            list_global_cases.append(path)\n",
    "        else:\n",
    "            # recursive keep going\n",
    "            recursive_merging(collection, index_set, path, list_global_cases) # recursive function\n",
    "        path = temp_path[:] # recursive돌아서 망가진 path 원래대로 복귀\n",
    "        \n",
    "# sorting based on start index\n",
    "final_collect_entry_anly = sorted(final_collect_entry_anly, key = lambda x: int(x[1]))\n",
    "\n",
    "# 오류확인 1. 처음과 끝의 경계는 아래와 같이 체크하면 된다. (중간에 있는 오류는 다음단계에서 확인한다.)\n",
    "assert(final_collect_entry_anly[0][1] == 0) # 처음이 index 0으로 시작하지 않을 경우.\n",
    "assert(final_collect_entry_anly[-1][2] == len_target_word) # 마지막이 word길이만큼의 index가 아닐 경우.\n",
    "\n",
    "### merging / combining all cases using recursive function\n",
    "list_global_cases, path = [], []\n",
    "# filtering only index0 (Starting point to recursive function)\n",
    "index0_entry_anly_set = [ele for ele in final_collect_entry_anly if int(ele[1]) == 0] # first position\n",
    "# recursive merging\n",
    "recursive_merging(final_collect_entry_anly, index0_entry_anly_set, path, list_global_cases)\n",
    "\n",
    "\n",
    "# 오류확인 2. 처음부터 끝까지 이어지는 하나의 case도 없을 경우.\n",
    "fullpath_check = False\n",
    "for i in range(0, len(list_global_cases)):\n",
    "    if list_global_cases[i][-1][2] == len_target_word:\n",
    "        fullpath_check = True\n",
    "assert(fullpath_check == True) # 오류 난다면 사전 lookup 문제이다\n",
    "\n",
    "# ### Sequence가 0에서부터 len_target_word-1까지 이어지지 않으면 삭제한다.\n",
    "# for i, seq in enumerate(list_global_cases):\n",
    "#     if seq[0][1] != 0 or seq[-1][2] != len_target_word:\n",
    "#         list_global_cases[i] = 'to-be-deleted'\n",
    "# # remove elements, which has 'to-be-deleted' in the first position        \n",
    "# list_global_cases = [elem for elem in list_global_cases if elem != 'to-be-deleted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['나', 0, 1], ['늘', 2, 4], ['ㄴ', 5, 5]], [['날', 0, 2], ['는', 3, 5]]]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_result = list_global_cases\n",
    "morph_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Alogorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check all words in \n",
    "# 그냥 형태소 분석의 출력 경우의 수 각각 확률을 구해주고 비교하는 거라면... 영어처럼 viterbi 수행해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_maxProb(tag_list, total_probState, i, j, prob_pTagcTag, output_probState):\n",
    "    \n",
    "    temp = [0] * len(tag_list)\n",
    "    for k, _ in enumerate(tag_list):\n",
    "    \n",
    "        # 주의: underflow 방지를 위해 단순 multiplication이 아닌, log sum을 실시한다.\n",
    "        # total_probState 변수는 이미 log화되어 있다.\n",
    "        \n",
    "        #print(total_probState[i-1][k], np.log(prob_pTagcTag[(tag_list[k], tag_list[j])]), np.log(output_probState[i][j]))\n",
    "        temp[k] = total_probState[i-1][k] * prob_pTagcTag[(tag_list[k], tag_list[j])] * output_probState[i][j]  # index 주의\n",
    "        \n",
    "   \n",
    "    max_prob = np.array(temp).max()\n",
    "    argmax_idx = np.where(np.array(temp) == max_prob)\n",
    "    \n",
    "#     print(max_prob)\n",
    "#     print(argmax_idx)\n",
    "    \n",
    "    index = argmax_idx[0][0] # (array([3], dtype=int64),) -> [0][0]해줘야 index 3가 추출된다.\n",
    "    \n",
    "#     print(index)\n",
    "#     print(tag_list[index], index, '-', tag_list[j], j)\n",
    "#     print('\\n')\n",
    "    \n",
    "    \n",
    "    return max_prob, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_pos_cases = []\n",
    "all_jointprob_cases = []\n",
    "prob_case = [0] * len(morph_result)\n",
    "tag_list = list(freq_Tag.keys()) # 주의: tag_list와 freq_Tag.keys()의 배열 순서는 다르다.\n",
    "\n",
    "for case in morph_result: # 형태소 분석 조합 경우의 수\n",
    "    output_probState = [[x for x in range(len(tag_list))] for y in range(len(case))]\n",
    "    total_probState = [[x for x in range(len(tag_list))] for y in range(len(case))] # total probability on the state\n",
    "    total_idxState = [[x for x in range(len(tag_list))] for y in range(len(case))] # optimal path (=[t-1] index) )\n",
    "    pos_case = ['n'] * len(case)\n",
    "    joint_prob = 0\n",
    "    \n",
    "    ###\n",
    "    ### Step 1: Assigning output probabliities\n",
    "    ### 일단 각 state에 대해 모든 출력확률을 구해놓는다\n",
    "    for i, morph in enumerate(case): # 형태소 경우의 수\n",
    "        cur_syllable = morph[0]\n",
    "        for j, tag in enumerate(tag_list): # POS Tag 경우의 수\n",
    "            output_probState[i][j] = prob_TagWord[(tag, cur_syllable)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    ### Step 2: Storing only maximum probabiliteis to each state by multiplying transition probs\n",
    "    for i, morph in enumerate(case): # 형태소 경우의 수\n",
    "        for j, tag in enumerate(tag_list): # POS Tag 경우의 수\n",
    "            #print('hi')\n",
    "            if i==0: # prev time에 START밖에 없으므로 max확률 찾을필요없이 그냥 다 저장한다.\n",
    "                pi_start_state = 1.0 # start state에 처음 있을 확률:100% 그냥 초기시작지점임.\n",
    "                total_probState[i][j] = prob_pTagcTag[('START', tag_list[j])] * output_probState[i][j]\n",
    "                #total_probState[i][j] = output_probState[i][j]\n",
    "                total_idxState[i][j] = 7 # 7:START. 첫 번째 state는 항상 START와 연결\n",
    "            else:\n",
    "                total_probState[i][j], total_idxState[i][j] = extract_maxProb(tag_list, total_probState, i, j, prob_pTagcTag, output_probState)\n",
    "                #print(total_probState[i][j], total_idxState[i][j])\n",
    "    \n",
    "    ###\n",
    "    ### Step 3: Storing argmax index for connecting previous time\n",
    "    temp = [0] * len(tag_list)\n",
    "    last_idx = len(case)-1\n",
    "    for j, _ in enumerate(tag_list):\n",
    "        temp[j] = total_probState[last_idx][j] * prob_pTagcTag[(tag_list[j], 'END')]\n",
    "        #temp[j] = total_probState[last_idx][j]\n",
    "    \n",
    "    # only for END tag\n",
    "    max_prob = np.array(temp).max()\n",
    "    end_idxState = np.where(np.array(temp) == np.array(temp).max())\n",
    "    \n",
    "    \n",
    "    end_idxState = end_idxState[0][0] # for extracting only index\n",
    "    #print(end_idxState)\n",
    "    #print(end_idxState, tag_list[end_idxState])\n",
    "    #print(total_idxState)\n",
    "    #print(total_idxState)\n",
    "    \n",
    "    ###\n",
    "    ### Step 4: Doing Back-tracking\n",
    "    tag_idx = -1\n",
    "    for i, x in enumerate(reversed(pos_case)):\n",
    "        last_idx = len(pos_case)-1\n",
    "        idx = last_idx - i # reversed index\n",
    "        \n",
    "        if idx == last_idx: # last syllable\n",
    "            pos_case[idx] = tag_list[end_idxState]\n",
    "            joint_prob = max_prob\n",
    "            tag_idx = end_idxState # for sending next time\n",
    "        else:\n",
    "            pos_case[idx] = tag_list[total_idxState[idx][tag_idx]]\n",
    "            #joint_prob *= total_probState[idx][tag_idx]\n",
    "            tag_idx = total_idxState[idx][tag_idx] # for sending next time\n",
    "            #print(tag_idx)\n",
    "            \n",
    "        #print(pos_case)\n",
    "            \n",
    "    #print(pos_case)\n",
    "    all_pos_cases.append(pos_case)\n",
    "    all_jointprob_cases.append(joint_prob)\n",
    "    \n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_jointprob_cases = np.array(all_jointprob_cases)\n",
    "indices = np.where(all_jointprob_cases == all_jointprob_cases.max())\n",
    "answer = np.where(all_jointprob_cases == all_jointprob_cases.max())\n",
    "answer_idx = answer[0][0] # 2개 이상이더라도 그냥 맨 처음에 있는 것을 argmax로 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['날', 0, 2], ['는', 3, 5]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_result[answer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SS', 'ETM']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pos_cases[answer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SS', 'NP', 'ETM'], ['SS', 'ETM']]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pos_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # dictionary filtering\n",
    "# cut_threshold = 30\n",
    "# freq_allTag = dict((k, v) for k, v in freq_allTag.items() if v > cut_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tag 빈도수\n",
    "# tag bigram 빈도수\n",
    "# word, tag 쌍 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전이확률\n",
    "# bigram확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
